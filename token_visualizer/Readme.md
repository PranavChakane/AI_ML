# ğŸ”¤ Tokenization Visualizer

Understand how LLMs "see" your text by visualizing how it is broken down into tokens â€” the fundamental units of input for models like GPT-2, BERT, RoBERTa, and LLaMA.

This beginner-friendly project helps demystify tokenization using Hugging Face tokenizers and an interactive Streamlit app. 

---

## ğŸ¯ What You'll Learn

| Concept                  | Why It Matters                                        |
|--------------------------|--------------------------------------------------------|
| **Tokenization**         | Core step in converting text into something models understand |
| **Byte-Pair Encoding**   | Common subword technique used in GPT-style models     |
| **WordPiece / SentencePiece** | Used by BERT, RoBERTa, LLaMA â€” helps with rare words |
| **Token IDs**            | Models process token IDs, not words                   |
| **Prompt Efficiency**    | Knowing how tokenizers work = faster, cheaper, better prompts |

---

## âœ¨ Features

- ğŸ§± Tokenize any sentence using GPT2, BERT, RoBERTa, or LLaMA tokenizers
- ğŸ§  Short intro to each tokenizer type
- ğŸ“„ Table view: tokens, token IDs, and lengths
- ğŸ“Š Bar chart: token length visualization
- ğŸ”¢ Displays total number of tokens (great for prompt optimization)

---

## ğŸ“¸ Screenshot

<sub>![alt text](image-1.png)</sub>
<sub>![alt text](image-2.png)</sub>
<sub>![alt text](image-3.png)</sub>
---

## ğŸ§± Project Structure

token_visualizer/
â”œâ”€â”€ app.py # Streamlit frontend
â”œâ”€â”€ utils/
â”‚ â””â”€â”€ tokenizer_utils.py # Token logic + tokenizer descriptions
â”œâ”€â”€ requirements.txt # Dependencies
â””â”€â”€ README.md # You're reading it!



---

## ğŸ§ª Sample Output

**Input:**  
`I'm learning how LLMs tokenize text!`

**Tokenizer:** `GPT2`

| Token   | Token ID | Length |
|---------|----------|--------|
| I       | 40       | 1      |
| â€™       | 821      | 1      |
| m       | 616      | 1      |
|  learning | 12166   | 9      |
| ...     | ...      | ...    |

ğŸ“Š Youâ€™ll also see a bar chart of token lengths and a total count like:
ğŸ”¢ Total Tokens: 11



---

## ğŸ”§ Installation

```bash
# Clone this repo
git clone https://github.com/your-username/token_visualizer.git
cd token_visualizer

# Create virtual environment
python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate

# Install requirements
pip install -r requirements.txt

â–¶ï¸ Run the App
Start the app locally with:

bash
Copy
Edit
streamlit run app.py
Then visit http://localhost:8501 in your browser.

ğŸ” Supported Tokenizers & Descriptions
| Tokenizer           | Description                                           |
| ------------------- | ----------------------------------------------------- |
| `gpt2`              | Byte-Pair Encoding (no special tokens)                |
| `bert-base-uncased` | WordPiece with lowercasing + special `[CLS]`, `[SEP]` |
| `roberta-base`      | GPT2-style with whitespace-aware tokens (â– prefix)    |


Use the tokenizer explainer panel in the app to get bite-sized intros!

ğŸ§  Why Tokenization Matters
"Language models donâ€™t read words â€” they read tokens."

Tokenization affects:

ğŸ’¸ API cost (more tokens = more $$)

ğŸ§  Context window length

ğŸ¯ Prompt formatting and accuracy

ğŸ§ª Fine-tuning and evaluation

Understanding it early sets you up for success in prompt engineering, RAG, and model deployment.

ğŸ¤ Credits
Built by Pranav 
Powered by:

ğŸ¤— Hugging Face Transformers

Streamlit

Altair



â­ Like This Project?
Give it a â­ on GitHub and check out the next project:
